Control software
====

This directory contains the code required for our robot and explains it

## Software and Language
To program our robot, we use the Arduino IDE and code an Arduino Sketch based on C++. Compiling and uploading our code is a simple process, as there are two buttons to help verify and upload our code. The click of this button will automatically compile and upload our code to the Arduino Nano microcontroller.

## Organization of Code
Our code follows Object Oriented Programming conventions, where we use one program type for each and every sensor and component of our robot. The library we use to interact with our electronic components are: Lego Mindstorms
### Chassis Class
In the chassis class, (ignition) blocks are used to control the drive and steering motors. These are "speed, timing", which is used to control the drive motor as the steering motor we are using.
The first method in our class is the motion function, which takes a whole speed accompanied by a turning angle of 10° to 20°. 
The second method in our class is the steering function, which steers our car at the correct angle. This method limits an angle variable to a specific range and then writes that range to the motor input pin.

### Camera Class
The Camera class interfaces with the PixyCam 2.1 through SPI (Serial Peripheral Interface) and gets the detected blocks. The member variable in this function is the "pixy" variable, an instance of the Pixy2 class provided by the "Pixy2.h" library. 


## Setup Code
In the setup of our program, we have a multitude of global functions and variables that we use in our main program. 

Arduino requires us to have a `void setup()` function, which runs once when the Arduino is turned on and a `void loop` function, which runs repeatedly until the Arduino turns off. In our code, we keep the setup of all the sensors and then wait until the button is pressed in the `setup` function, and we use the `loop` function to run our code. The first function that we use in our code is the `void delay_2` function which takes in an integer value and delays for that amount of milliseconds while updating the gyro sensor. We use this function instead of the default `delay` function since it allows our gyro sensor to constantly update. We then have two separate functions for each challenge. The `void open` function contains the code for the open challenge, and the `void obstacle` contains the code for the obstacle challenge. We implement these functions in the `loop` function by commenting on which function we are not running and uncommenting the function that we are running.

## Open Challenge Code
In the open challenge, we implement gyro and IR following, which uses readings from the sensors to ensure a straight path. Before we pass the first corner, the robot does not know which direction it is travelling in, so it gyro follows. We implement gyro following in the following code where it finds the difference between the target angle and the current angle and multiplies that error to get the steering. `err = targetAngle - gyro.getAngle(); steer = err*2.0;` To set our direction, we run this code constantly which checks if the direction is not set, it will reset it to whatever the RGB sensor is seeing. `if (dir==0) { dir = rgbSense.getColor(); }` The next part of our code checks if the RGB sensor sees the same color as the direction that the robot is travelling and it has been 2 seconds since the last line was seen and increments to cornerCount, sets the end time to after 3 seconds, flags the cornerDetected variable as true, changes targetAngle, and resets cornerScanDelay. We do this in the following code. `if (rgbSense.getColor()==dir && dir!=0 && millis()-cornerScanDelay>2000) { delay_2(60); cornerCount++; endTime = millis()+3000; cornerDetected = true; targetAngle += (dir == 1 ? 90 : -90) cornerScanDelay = millis(); }` We then have a code to check if the cornerDetected variable is true, the robot is close to the wall, and the robot is not wall following, and if all the conditions are true, the robot will take a turn based on which direction the robot is travelling. We then check if the robot is close to the walls, and if it is, we set the caughtOnWall variable to true so that the robot will start wall following. We then check if the robot is too close to the walls on the side and turn if so. We then check if we are supposed to wall follow, and wall follow if so. The wall following is done with the following code. `if(caughtOnWall) { if(dir == 1) { err = 20.0 - irSensors.getFarDistance(2); steer = err * -2.0; } else if(dir == 2)  { err = 20.0 - irSensors.getFarDistance(0); steer = err*2.0; } if (steer>15){ steer = 15; }else if (steer<-15){ steer = -15; } }` We then make sure that the steering is not greater than 30 degrees and we set the steering and speed if it is not time to stop the run.

## Obstacle Challenge Code
For the obstacle challenge, we use a variety of sensors, including the RGB sensor, IR sensors, gyro, and camera. Our code uses these sensors to avoid obstacles and turn effectively. This code is split into two main parts. The first part is the normal navigation, and the other part is the turning logic. In the normal navigation logic, we start off by checking if the time since the corner turned is less than 2 seconds and get the closest signature based on that. We then set the prevObj variable to the current block if the block is close enough to the robot in the following code. `if (closeBlock.m_y>150 && closeBlock.m_signature<=2) { prevObj = closeBlock; }` We then have the same direction detection logic as the open challenge. After that, we have the block following which we do if we see a block, not a line or nothing which is done in the following code. `if (closeBlock.m_signature<=2) {  if (closeBlock.m_signature==1) { target = (207-closeBlock.m_y)/1.5 + 5; }else { target = 310.0-(207-closeBlock.m_y)/1.5; } err = target - (int)closeBlock.m_x; steer = err*kP; if (steer>30){ steer = 30; }else if (steer<-30){ steer = -30; } }` If there is no block sensed, we run the wall following code which is done with the following code. `err = 60 - irSensors.getFarDistance(dir == 1 ? 0 : 2); steer = err * (dir == 1 ? 1.0 : -1.0) * 1.0; if (steer>10){ steer = 10; }else if (steer<-10){ steer = -10; }` We then have some code which makes sure we don't crash int anything on the side which is done by using the IR sensors to detect the distance from the sides and turn accordingly. We do this in the following code. `if (irSensors.getFarDistance(0) <= 15) { if(closeBlock.m_signature != 1) { steer = 20; } else if (irSensors.getFarDistance(0) <= 12)  steer = 30; }else if (irSensors.getFarDistance(2) <= 15) { if(closeBlock.m_signature != 2) { steer = -20; } else if (irSensors.getFarDistance(2) <= 12)  steer = -30; }` This code checks the side sensors and the current block that the robot is seeing to determine which direction and how much to turn. After this is done, we have an if statement to identify if our robot is going to crash from the front, and if so, the robot will move backwards for three seconds and resume the program. The last part of normal navigation is where the robot sets the steering and speed if the time is not up. For the turning logic, we have an if statement that checks if it is time to turn and runs the code accordingly. The first thing that happens in the if statement is that we increment to cornerCount and check if we need to set the stopping time. We then check if the last block passed was red, and the robot finished the second lap and ran the 180-degree turn where the robot uses the gyro to go close to the wall and turn. The gyro following is done in this code: `while (irSensors.getDistance(1)>50){ err = targetAngle + 170 - gyro.getAngle(); steer = err*2.0; if (steer>20){ steer = 20; }else if (steer<-20){ steer = -20; } delay_2(1); chassis.steer(steer); gyro.updateGyro(); }` and the turning amount changes based on the direction we are travelling in. If we are supposed to run a normal turn, we check if we need to turn on the outside, inside, or neither and run based on that. For all of these turning cases, we use the gyro sensor to turn. To determine which case to go into, the robot takes eight frames from the camera and checks if a block appears in at least 2 of those frames and based on the block's color, we determine which case to go into.
